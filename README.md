## 项目介绍

### 大纲

![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231124112829310-1199337557.png)


**功能模块**：

![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231124111350680-396366612.png)

**一、数据抽取模块**
- 用于从数据源的数据库中抽取数据，生成xml文件；

- 这是一个通用的功能模块，通过配置脚本，就可以从不同的表中抽取数据；
- 支持全量抽取（每次抽取全部的数据）和增量抽取（每次只抽取新增的数据）；
- 支持Oracle、MySQL和SQL Server数据库。 你做的是Oracle数据库，其它数据库的由同学/同事负责。

**二、基于ftp协议的文件传输模块**
- 采用ftp协议，从ftp服务端下载/上传文件；

- 这是一个通用的功能模块，通过配置脚本，就可以实现文件下载/上传任务；
- 把开源的ftp客户端库ftplib.c封装成C++的类，使用起来更方便；
- 支持增量传输的功能，每次只下载/上传新增的文件；
- 适用于不同的业务系统之间传输文件。

**三、基于tcp协议的文件传输模块**
- 自定义tcp通讯协议，实现了文件传输的服务端和客户端程序，支持文件下载/上传功能。

- 这是一个通用的功能模块，通过配置脚本，就可以实现文件下载/上传任务；采用了异步通讯的方式，传输文件的效率非常高，远超过ftp协议；
- 支持增量传输的功能，每次只下载/上传新增的文件；
- 适用于同一个业务系统的多个服务器之间传输文件。

**四、数据处理模块**
- 数据处理是指把各种格式（编码）的文件转换成xml文件；

- 数据源文件的格式千奇百怪，所以，数据处理模块由若干个小模块组成，每个小模块负责处理一种格式的文件。

**五、数据入库模块**
- 把数据抽取模块和数据处理模块生成的xml文件存储到Oracle数据库中；

- 这是一个通用的功能模块，通过配置脚本，就可以实现不同种类数据的入库；
- 根据xml文件名识别数据种类（需要入库的表），查询Oracle的数据字典，得到表的字段名，用字段名解析xml文件，获取数据，把数据入库到表中。

**六、数据管理模块**
- 大部分的数据可分为热数据和冷数据，在气象行业中，历史数据属于冷数据，数据管理模块的功能是把冷数据删除、备份或归档。

- 数据管理是通用的功能模块，通过配置脚本，就可以实现对不同数据表的管理

**七、数据统计模块**
数据统计功能的业务相关性比较强，不同的数据有不同的统计需求。数据统计的主要目的是对源始的数据进行二次加工，生成新的数据集、数据产品、报表。

**八、数据同步模块**
- 数据中心的数据库是一个集群，核心数据库采用RAC，应用数据库是若干个单实例，核心数据库负责处理数据，应用数据库负责提供数据服务。

- 数据同步是指把核心数据库中的数据同步到应用数据库中；
- 这是一个通用的功能模块，通过配置脚本，就可以实现不同表的同步；
- 利用了Oracle的dblink和数据字典；
- 支持刷新同步，全表刷新和按条件刷新；
- 支持增量同步，只同步新增的数据；
- 支持不同表结构的同步（源表和目的表的表结构不同）；
- 可以指定同步数据的条件。

**九、数据访问接口模块**
- 采用http协议，为业务系统提供数据服务；

- 这是一个通用的功能，通过配置参数，就可以实现不同的数据访问接口；
- 服务端程序采用了线程、线程通讯、管道、智能指针和epoll等技术；
- 数据访问接口性能瓶颈在Oracle数据库，并发性能在3-5千/秒左右。
- 服务端程序的总体结构如下：

**项目重点和难点**：
服务程序的稳定性；

数据处理和服务的效率；

功能模块的通用性


## 通用模块
### 框架
**封装细节**：
- 字符串：最实用的：分割字符串，常用于分割SQL 配置文件、正则表达式常用来匹配文件，解析

xml字符串常用于解析参数或者带xml文件，数据入库的xml配置文件，参数过多使用xml解析
第一步 模块让用户在配置文件输入自己的字段以及表名更新时间即可实现模块初步通用

第二步是xmlSQL动态绑定

- 输入输出： 支持格式化输出字符串常用于日志

- 时间： 字符串和整数的时间能够互相转换用于日志记录时间，简洁的定时器可以用于简单测试插入或者通信的代码段性能以及对比文件是否历史文件，还可用于守护进程心跳服务
- 日志：日志程序的运行时间，运行阶段的状态，处理数据的情况，很方便进行排查错误，多线程日志加锁，超过日志大小切换切换日志文件，日志和文件命名：时间和文件相关参数拼接
- 网络通讯：封装了socket通讯的基本客户端服务端编写，输入必要的参数，简化了流程，粘包问题，
	+ 消息长度标识符：在消息的头部添加一个固定长度的标识符，用来表示消息的长度。接收端在读取数据时，先读取标识符，然后根据标识符指定的长度读取消息数据，从而避免粘包问题。设置超时机制
- 文件目录：最主要的线程冲突，文件操作没有锁机制，根据生成临时文件并且改名的方式。逐级创建删除目录，不需要一直判断权限。
- 进程通讯：信号量用于互斥锁和生产者消费者模型，进程心跳，把当前进程的信息加入共享内存进程组中，更新共享内存进程组中当前进程的心跳时间，对比当前系统时间，超时时间释放
- 封装ftp：ftp使用github上现成的ftplib库进行个性定制保留项目需要的api，用户不需要特别安装ftp客户端，额外保存失败的原因记录到日志当中，能够额外将上传下载获取列表以文件的形式额外保存起来
- 封装oracle，orcale的oci库很麻烦，直接在网咯上找到大佬的oracle库封装一下就行了

上述提到的好处在后续相关模块会提及补充

## 通用模块
![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231123084855040-1786487612.png)
### 服务程序的调度模块

```
/etc/rc.local 中增加开机启动程序脚本 用chmod +x /etc/rc.d/rc.local

/project/tools1/bin/procctl 30 /project/tools1/bin/checkproc

su -wuhm -c "/bin/sh /project /project/idc1/c/start.sh"
```
服务器程序运行在后台，没办法每时每刻检查，尤其网络通讯容易死，

周期性启动服务程序或shell脚本运行在后台，ps -ef检查后台

参数：调度程序 运行周期 被调度的程序 被调度程序的参数，关闭所有的io，忽略所有信号，防止被误杀，只能kill -9强行杀

生成子进程父进程退出使其给进程1托管在后台，把子进程孩子通知信号打开可以被父进程wait，用一个数组存放被调度名和参数，在循环中，子进程再调用子进程，exec被调度的程序，如果失败会被父进程wait，sleep运行周期一会后重新fork一个。

#### 服务程序的监控和调度-进程的心跳
开辟一个共享内存表示心跳信息，结构体存放进程id，进程名称，超时时间，最后心跳。

创建/获取共享内存，将共享内存连接到当前进程的地址空间，进程信息添加进去，多线程会冲突，更新心跳时间，超时退出（由守护模块），删除共享内存当前的进程心跳信息，共享内存从当前进程分离。再进行进一步封装

#### 服务程序的监控和调度-守护模块
打开日志文件,创建/获取共享内存,将共享内存连接到当前进程的地址空间,遍历共享内存中全部的记录，用当前时间减去进程心跳时间，如果进程已超时，终止它,把共享内存从当前进程中分离

#### 服务程序的监控和调度-运行策略（使用方法）
全部的服务程序启用心跳机制，调度模块和守护模块除外

全部的服务程序用调度模块（procctl）启动。

启动守护模块（checkproc），如果服务程序超时，终止它。

使用脚本文件快速启动服务程序先启动守护进程和停止调度程序的脚本，先停止调度程序，停止其他服务程序，如果其他程序会因为异常无法退出，则等待五秒使他真正退出，再强行杀死。

超级用户启动守护进程防止为误杀

### 清理文件和压缩文件
用于压缩或者删除历史的数据文件和日志，把目录及子目录中timeout历史时间与现在的时间比较天之间的匹配matchstr文件全部压缩或者删除

删除调用remove函数，压缩调用系统的shell压缩命令，用调度程序定时启动

删除运行时间很短不需要加心跳，压缩这种需要压缩很大的文件需要加心跳


## 文件传输模块
### 基于ftp协议的文件传输系统
**为什么使用ftp：** 
简单通用，创建ftp服务很容易，既可以使用现成的ftp客户端服务端，也可以基于ftplib封装自己实现个性化，适合在内部网络进行文件传输，

ftp协议最底层是tcp报文，如果从socket开始编程，工作量巨大，寻找ftp客户端的开源库 (ftplib)，封装成简单易用的类cftp，可实现 ftp的功能，只需要输入ip地址端口用户名主机名

ftp的功能都可以通过将配置参数传入给xml解析实现，是一个通用的模块，
将ftp客户端服务端作为本机只是存储目录不同以实现两台虚拟机的效果

- 自己封装ftp客户端的意义是学习和提高技能

- 使用多线程或异步IO：提高FTP客户端的并发处理能力，
- 文件校验错误（上传下载前后获取文件时间和大小，如果时间不一致就放弃传输），使用SSL/TLS协议加密传输
- 断点续传：支持断点续传功能，即当文件传输中断后，可以从中断处继续传输，而不需要重新开始传输整个文件。
- 日志记录：记录FTP操作的日志，包括连接日志、上传下载日志等，以便进行故障排查和操作追踪。
- 方便维护管理：客户不要安装客户端，模块化设计将不同功能的代码模块化，代码规范化易于阅读和理解，统一更新保证客户端的稳定性和一致性，方便其他开发人员理解和维护

**流程**
1. 结构体st_fileinfo中包含文件名filename，文件时间mtime

2. 容器vlistfile2存放服务器待下载文件信息；容器vlistfile1存放已经传输完成的文件信息；
3. 容器vlistfile3存放本地以及下载的但服务器中还有的文件；容器vlistfile4存放服务器中新增文
4. listfilename中存放服务器文件；okfilename中存放已经传输完成的文件strremotefilenamebak备份目录
5. 容器2、3、4只会遍历使用链表，判断容器是否新增修改使用map存放文件名和文件时间

ftp.nlist()**方法列出服务器目录中的文件，结果存放到本地文件listfilename中
ftp.nlist()方法获取到的list文件加载到容器vlistfile2中
读取本地文件中的一行文件名，MatchStr匹配满足条件的文件

当ptype为1，checktime为true时获取ftp服务器文件时间放入ftp.m_mtime中
1. 如果ptype==1
2. 加载okfilename文件中的内容到容器vlistfile1中

3. 比较vlistfile2和vlistfile1，得到vlistfile3和vlistfile4
4. 把vlistfile4中的内容复制到vlistfile2中：vlistfile2.clear(); vlistfile2.swap(vlistfile4);

如果ptype==1，把下载成功的文件记录追加到okfilename文件中

如果ptype==2，使用ftp.ftpdelete()删除服务器文件

如果ptype==3，使用ftp.ftprename()服务器文件转存到服务器备份目录

**文件上传**
功能一样，服务端逻辑改成客户端逻辑

### 基于tcp协议的文件传输系统
![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231123110901641-1070407622.png)
ftp 的效率比较低（tcp通讯采用一问一答的方式，交互次数太多)，适用于系统之间传输文件，开通ftp服务就可以直接从目录获取，类似校园网交作业，有权限控制，tcp效率高，通讯方式自定义

**tcp长链接**：
**心跳机制**：发送心跳包（xml表示）来保持连接的活跃状态，发送任意符号心跳报文保持活跃，在socket的read请求报文和回应报文里面加上超时时间

文件上传下载客户端 传输服务端模块需要自己写，功能一样，tcp通讯协议自己写，一边该一边测试

客户端向服务端发起连接请求，发送互相回应的心跳报文这个是和业务逻辑分开单独放的

1. 服务端一个负责监听一个处理客户端连接的处理上传和下载的业务逻辑，客户端发送自己的登录报文表明下载还是上传并且附带自己本地目录参数、
2. 登录成功后，客户端打开本地目录获取文件清单，文件名文件大小文件时间拼接报文发送给服务端，服务端接收判断客户端下载上传类型成功就确认响应报文
3. 客户端就开始发送报文，为了防止粘包会用拼接一个数据头记录文件发送的大小以及每次可以发送文件的大小
4. 根据类型调用响应的主函数，接收报文判断是心跳报文就回应，如果是上传或者下载文件报文，解析并且接收文件内容

6. 回应报文告知服务端接收文件内容成功并且通知本地文件删除或者备份，循环直到客户端文件全部发送完成

使用epoll改善客户端服务端程序实现异步通讯加快效率，将接受报文和发送报文和，监听客户端处理客户端分隔开来，为他们注册特定的读事件和写事件

本质上都是传输文件，代码逻辑和参数客户端ptype=2改一下就可以了


## 数据核心模块

#### Oracle数据库集群方案
RAC（Real Application Cluster实时应用集群）由多个服务器加共享存储，一般两个节点，高可用方案，写入数据的性能比单实例略低，读略高。

Data Guard数据同步，把日志文件从源数据库传到目标数据库，从而达到数据库级别的高可用方案，相当于mysql的主从复制，mysql叫master和slaver，oracle叫primary和standby，用于异地备份，读写分离

OGG（Oracle Golden Gate），捕捉远端数据库的日志，把发生变化的数据提取出来，生成文件发送给目标端，目标端解析文件，数据同步效率比较高，非常灵活，可以配置需要同步的表

OGG Vs 数据同步子系统
1. OGG收费
2. OGG从日志中抽取，因此对源数据库没有压力，对表的设计没有要求。但数据同步子系统要求要有自增字段，且不能插入删除
3. 数据同步子系统部署简单，批量操作数据效率更高

### oracle基础

封装的oracle，将数据库连接和事务操作封和SQL语句封装成类。通用操作，登录数据库，连接数据库对象，准备SQL语句，绑定参数，效率比静态绑定变量高，特殊字符方便处理，避免sql注入，并且可以通过xml动态绑定更加通用，执行sql，放到结果集中，执行提交事务，错误信息会存储在对象中可以打印，析构会自动断开连接

**SQL注入**：
在使用动态绑定数据库查询参数时，参数值是以变量形式传递给查询语句的，而不是直接拼接到查询语句中。这种方式可以避免SQL注入攻击，因为参数值不会被解释为SQL代码，而是被视为数据

**为什么使用oracle**
- MySQL: 免费，性能、安全性较低，适用于不重要的、数据量小 (单表记录数不超过一百万) 的项目。

- Oracle: 收费，性能卓越，安全可靠，适用于重要的、数据量大 (单表记录数几十亿) 的项目。
- 互联网公司：数据库以MySQL为主，免费是关键，性能（可用redis 代替) 和安全性 (用分布式解决) 不是重点。钱存放在Oracle中。
- 传统行业：数据库以Oracle为主，不重要并且数据量小的项目才用 MySQL。

**oracle特性**：
序列存储器是Oracle数据库中的一种对象，它可以生成唯一的递增数字，可以在多个表中使用，并且可以在不同的事务中保持一致性。使用序列存储器可以避免在插入新记录时出现重复的键值，递增字段但是，如果在多个表中使用递增字段，可能需要额外的处理来保证唯一性，并且在不同的事务中可能会出现冲突。

**MySQL高可用方案的不足：**
Master（写）和Slave（读）的表结构和数量必须一致

非主从关系的数据库不能进行复制

不够灵活，效率也不高

（Oracle dblink）把远程数据库同步到本地（实现数据库中的互相访问）

### 数据抽取

全量抽取根据主键也就是编号和时间戳，增量需要加上序列生成器的最大字段

从数据库的表中查询数据，一个通用的模块将抽取的SQL语句参数化，都可以参数化成xml格式不写死动态然后解析动态绑定在SQL模块抽取，把结果集保存到xml文件中。

- **全量抽取**：按查询条件抽取表中全部的数据（表中的数据会修改）,结果集字段一条条改变前后缀拼接，生成xml文件，

- **增量抽取**：从文件或数据库中获取上次已抽取数据的增量字段的最大值；（如果是第一次执行抽取任务，增量字段的最大值为0））绑定输入变量（已抽取数据的增量字段的最大值）；
	+ 获取结果集的时候，要把增量字段的最大值保存在临时变量中；抽取完数据之后，把增量字段的最大值保存在文件或数据库中。

xml文件将用于入库，如果文件太大，数据库会产生大事务，需要分批抽取
根据数据量做到增删改查，不要删除，否则数据库引擎可能需要重新组织数据存储，索引维护，事务日志和回滚段，数据一致性，只修改。


### 数据入库
把从各政府部门抽取出来的xml 文件入库到共享平台的数据库中。 数据集有几千种，为每种数据写一个入库程序吗?表结构不同，配置一个入库参数文件

![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231123144411860-1109038260.png)
![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231123143837625-1188164189.png)
![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231123143941327-164885278.png)
![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231123144122496-949558670.png)

#### 数据处理模块
![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231123145404271-786456013.png)
**数据处理：**
任务：把各种格式的数据文件转换为xml格式的文件，再交给入库程序

**步骤**：1. 读取目录中的文件 2.解析文件的内容 3.生成xml文件格式的文件

使用了github的开源模块

### 数据管理模块
在实际工作中，最重要的是当前数据，n天前的历史数据价值下降。

如果表的记录超过亿，插入和查询的性能略有下降，迁移和备份也不方便。

如果历史数据不需要保存，可以删除它；如果历史数据仍有价值，可以把若干天之前的数据备份到历史表中。

**业务需求**
- 如果数据量不是特别大，可以只用一个历史表

- 如果数据量特别大，可以一年一个历史表，或一个月一个历史表。
- 如果数据量很大，会产生大事务，冲击数据库系统。
- 正确的做法是采用蚂蚁搬家似的方法，每执行一次SQL语句，只操作数百行记录。

根据清理数据的条件，把表中的唯一键字段查询出来；以唯一键为条件，删除表中的记录；为了提高效率每执行一条SQL语句删除100或200条数据。

根据主键从删除表查询需要删除的记录然后执行sql批量删除。

从原表获取数据插入到目的表，批量插入表，从迁移表删除记录

### 数据同步模块
共享平台的核心数据库使用RAC，不提供服务，而是将数据分类同步到不同业务库中,业务库提供不同的访问借口
RAC服务器采用IBM3850内存：128G，硬盘：300G
存储服务器EMC 100T
应用服务器IBM3650

历史 实时 内网 外网库，一个不够就多加几个库，数据同步使用GG或者数据同步接口

通过oracle之间建立dblink数据库连接可以实现数据同步，主从机，数据库dblink自己实模拟两台服务器

对于没有建立数据库链路的程序，先数据抽取再数据处理再数据入库，否则可以直接数据抽取

表结构应该是一样，如果远程表结构不完全相似，没有时间字段，用时间字段填充，没有编号就用序列生成器，本地表参数多一点，如果没有远程表字段为空就用多余的字段添补


远程表的字段通过数据字典查询插入查询语句当中，本地表本身已知



**三者功能都是依次递进的**
不分批刷新：不分批刷新适合数据量不大，远程表的增加删除修改可以同步到本地表，删除本地表中的老数据，再根据本地数据字段查询数据字典对方表的主键，确认与对方的字段是否大致相似不想死就填充，远程表满足条件的数据插入到本地表

分批刷新：适合数据量很大适合增加修改不能删除，同步数据太大会产生大事务，对本地数据库冲击,
在分批刷新的基础上，从远程表中查找需要同步记录的key字段的值，达到一定数量(maxcount) 执行delete和insert语句

增量同步：从本地表查询已经同步记录的最大id每次保存在数据库当中，从远程表中查询需要同步的记录，再把记录分批插入

**触发器实现同步**
创建一个触发器，当本地表有数据插入或更新时，触发器会自动执行同步操作。在触发器中，可以使用存储过程来查询本地表已同步记录的最大id，并从远程表中查询需要同步的记录。然后将这些记录分批插入到本地表中，实现数据同步。

触发器对源数据库压力很大 插入日志表 从源表提取数据


### 数据服务总线
#### 前置知识
业务系统使用SQL直连数据中心的应用数据库，可以任意访问数据，适合政府

业务系统使用HTTP协议在浏览器输入url获取数据中心的应用数据库的数据，适合公众

请求方法只用到了GET，hhtps加密更加安全，在浏览器url旁边会显示一把锁，因为前者麻烦所以使用了http

使用c++模拟实现http的三种方式![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231122145648960-879374423.png)**使用长链接**一问一答的方式字段：值connection：kepp-alive保持活跃
右键网页查看源代码也可以查看响应报文的内容只不过没有状态行需要在network里面看

![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231122145655898-1472945461.png)请求的内容按固定格式拼接成字符串
检测请求报文的字符串格式：在虚拟机上运行一个常见的scoekt程序绑定端口，接受的报文打印到控制台，在浏览器url上输入ip和端口

get / 根目录，host必填，端口缺省80小心占用，url？前html表面来源页，后面&间隔表示参数，参数值格式可以自己定义，模拟标准的

![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231122145703983-667328165.png)

**模拟浏览器**：模拟像知乎发送一个格式标准的请求报文,注意报文里面的回车符和换行符，然后将相应报文打印到控制台上，网页内容太多的时候需要while读完不然只会读取一部分，输出的html文件对比观察学习

**相应报文的重点**是状态码，和相应报文的长度Content-Length可以判断相应是否结束还是分块传输

**模拟服务器**：发送准备好的相应文件。接受连接请求，生成相应报文固定状态行，报文长度用文件参数填充，发送相应报文后，再发送文件，限制每次发送文件buffer片段的长度

c++开发网页很麻烦，但是后端发送并且生成csv json这些文件效率最高，有socket

**简易的数据访问接口**：

接收http请求，解析请求参数，从Oracle的T_ZHOBTMIND1表中查询数据，返回给客户端

接口参数一般是：http，ip地址和端口、用户名和密码（数据库）、接口名（哪个表）、url获取那个位置的资源 传递给位置具体的参数（具体字段主键站点编号、起始时间和截止时间）

接收请求报文  把相应报文发送给客户端 解析请求报文参数从表中查询数据返回客户端
解析请求报文长度主要是依次查找参数名的位置然后阶段存放在字符串当中

然后使用封装的oracle连接数据库，连接前检测用户名密码是否合法，查询SQL语句使用解析报文出来保存在字符串里面的参数进行动态绑定，再返回一行一行的执行结果集

![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231122164548960-439406171.png)
**参考深圳数据开发平台的前端界面**
用户基本信息表有用户名和密码关键，用户接口由固定的权限限制，区分公安和公众，然后调用接口接口的日志记录编号和用户名，最后接口参数表有接口代码访问哪个接口，再从数据种类表里面找，表设计太业务，别人设计的，有现成SQL，先创建再插入

查询条件使用动态绑定报文解析出来的参数这样就不会把SQL写死，可以按照不同要求查询数据，输入接口和请求的参数要匹配

表之间主外键关系仔细看图	

#### 正式数据接口

接受线程接受客户端的请求报文，放到接收队列中，工作线程从接收队列获取请求报文并且解析，执行sql语句获取数据生成相应报文交给发送队列，发送线程获取相应报文发送给客户端。
接受线程只有一个，工作线程多个，发送线程只有一个
接收线程负责建立连接 生成请求报文 关闭连接
接受线程只有一个，工作线程多个，发送线程只有一个

代码详细（有必要）![image](https://img2023.cnblogs.com/blog/3124760/202311/3124760-20231122201525970-1635317772.png)
一个客户端的结构体里面存放客户端的ip地址，最后一次的活动时间，和接受发送的缓冲区
接收和发送队列内容一样，共用一个结构体，里面是客户端的socket，和接收发送的报文
工作线程和接收线程类，用一个队列来存放接受队列结构体，配合一个互斥锁和条件变量,用一个队列来存放发送队列，用一个互斥锁控制，实现生产者消费者模型
用一个管道来使得工作线程来通知接收线程
哈希表来存放客户端对象，状态机key存放客户端fd，value存放客户端结构体

接受线程初始化一个监听的socket装备epoll读事件，处理连接客户端，装备读事件，添加epoll
如果是客户端连接有事件，有报文则接收，待处理流程，或者连接断开，断开从状态机删除，
根据报文格式如果有两个回车换行则收到一个完整的http报文，等客户端接收或者发送缓冲区完整报文再发送，以此解决粘包分包问题，然后再把报文入队交给工作线程

加锁，把客户端的socket和请求报文放到请求队列中，使用智能指针来存放这两个对象可以避免结构体队列进进出出引发的内存拷贝从，然后再通知工作线程处理接收队列报文

加锁，工作线程需要先连接数据库，这个数据库参数通过xml脚本解析进来，工作线程等待生产者也就是接收线程的唤醒信号出队报文，解析请求报文、判断用户名ip接口权限，获取接口对应的参数，执行查询数据的SQL语句、拆分字段生成xml格式、放到结果集当中，生成响应报文，再把客户端的socket和相应报文放到发送队列，发送给客户端 （注：览器能自动解析xml格式数字变成普通文本）

发送线程创建epoll监听socket事件发生，如果epollwait阻碍，发送队列有数据，无法通知发送线程，工作线程使用管道通知发送线程，把管道加入发送线程的读事件监听，工作线程把响应报文放入发送队列后向管道写入一个字节起到通知的作用。发送线程如果监听到的事件是管道则处理发送队列出队的全部元素，添加到客户端的发送缓冲区，客户端关注加入到写事件，这时候就会触发socket的写事件，再把报文发送给客户端

**身份认证**：
分配用户名和密码 分配唯一的key

黑名单、白名单、绑定ip（只能通过某ip地址访问）这些在数据库中有设计表

使用protobuf进行公钥加密

**优化方案**
主动断开空闲的tcp连接（节省资源、防止被攻击）

防止socket的接收缓冲溢出 (防止被攻击)，如果请求报文不判断，会导致接收缓冲区爆掉，分配内存失败的爆掉错误

不同的工作线程可以连不同的数据库，提升性能 (不能采用redis)
